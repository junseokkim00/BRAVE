{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/opt/anaconda3/envs/rp2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 90447/90447 [00:15<00:00, 5758.11 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset('hotpotqa/hotpot_qa', 'fullwiki', trust_remote_code=True)\n",
    "train_dataset = ds['train']\n",
    "def preprocess(example):\n",
    "    context=\"Context: \"\n",
    "    question = example['question']\n",
    "    for title, passage in zip(example['context']['title'], example['context']['sentences']):\n",
    "        sentences = \" \".join(passage)\n",
    "        context+=f\"{title} : {sentences}\"\n",
    "        context+=\"\\n\"\n",
    "    context+=f\"question: {question}\\nevidences: \"\n",
    "    example = {\n",
    "        'input': context,\n",
    "        'answer': str(example['supporting_facts']['title']),\n",
    "    }\n",
    "    return example\n",
    "# ['question', 'answer', 'level','supporting_facts', 'context'])\n",
    "\n",
    "train_dataset_mapped = train_dataset.map(preprocess)\n",
    "train_dataset_mapped = train_dataset_mapped.select_columns(['input', 'answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset_mapped[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/Users/andrew/opt/anaconda3/envs/rp2/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def tokenize(batch):\n",
    "    batch_encoding = tokenizer(\n",
    "        batch['input'],\n",
    "        padding=\"longest\",\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    target_encoding = tokenizer(\n",
    "        batch['answer'],\n",
    "        padding=\"longest\",\n",
    "        return_tensors = 'pt'\n",
    "    )\n",
    "    labels = target_encoding.input_ids\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    return {\n",
    "        'input_ids': torch.tensor(batch_encoding['input_ids']),\n",
    "        'attention_mask': torch.tensor(batch_encoding['attention_mask']),\n",
    "        'labels': torch.tensor(labels)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/90447 [00:00<?, ? examples/s]/var/folders/0r/6ncbx7693d96pr3xr0xpgt8w0000gn/T/ipykernel_70161/3795642879.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(batch_encoding['input_ids']),\n",
      "/var/folders/0r/6ncbx7693d96pr3xr0xpgt8w0000gn/T/ipykernel_70161/3795642879.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask': torch.tensor(batch_encoding['attention_mask']),\n",
      "/var/folders/0r/6ncbx7693d96pr3xr0xpgt8w0000gn/T/ipykernel_70161/3795642879.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(labels)\n",
      "Map: 100%|██████████| 90447/90447 [04:41<00:00, 320.89 examples/s]\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset=train_dataset_mapped.map(tokenize, batch_size=4, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = encoded_dataset.remove_columns(['input', 'answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
